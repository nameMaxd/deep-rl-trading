#!/usr/bin/env python3
"""
üî• MLP v16 –° TQDM –ü–†–û–ì–†–ï–°–°-–ë–ê–†–ê–ú–ò!
üéØ –ì–ª—É–±–æ–∫–∏–π MLP baseline —Å –∂–∏–≤—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
"""

from src.rl.env import Env
from src.rl.agent import Agent
import numpy as np
from tqdm import tqdm
import time

def main():
    print("üî• MLP v16 –° –ñ–ò–í–´–ú TQDM –ü–†–û–ì–†–ï–°–°–û–ú!")
    print("üéØ –ì–ª—É–±–æ–∫–∏–π MLP baseline —Å residuals!")
    print("=" * 60)
    
    # MLP –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = {
        'episodes': 160,
        'trading_period': 80,
        'window': 35,
        'commission': 0.0002,
        
        # MLP –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        'embeddings': 32,  # –ü—Ä–æ—Å—Ç–æ–π –¥–ª—è MLP
        'heads': 2,
        'layers': 2,
        'fwex': 128,
        'dropout': 0.2,
        'neurons': 128,
        'lr': 0.001,
        'epsilon': 0.8,
        'epsilon_min': 0.01,
        'epsilon_decay': 0.995,
        'gamma': 0.95,
        'memory_size': 10000,
        'batch_size': 256,
        'update_freq': 5
    }
    
    print(f"üìä MLP –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config['episodes']} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è
    train_env = Env(
        csv_paths=["GOOG_2010-2024-06.csv"],
        fee=config['commission'],
        trading_period=config['trading_period'],
        window=config['window']
    )
    
    oos_env = Env(
        csv_paths=["GOOG_2024-07_2025-04.csv"],
        fee=config['commission'],
        trading_period=config['trading_period'],
        window=config['window']
    )
    
    print(f"üìä MLP –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
    print(f"   Train: {train_env.stock.obs_space.shape}")
    print(f"   OOS: {oos_env.stock.obs_space.shape}")
    
    # –°–æ–∑–¥–∞–µ–º MLP –∞–≥–µ–Ω—Ç–∞
    agent = Agent(
        obs_space=train_env.stock.obs_space,
        **{k: v for k, v in config.items() 
           if k not in ['episodes', 'trading_period', 'window', 'commission']}
    )
    
    print(f"üî• MLP –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω:")
    print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {config['embeddings']} emb, {config['layers']} layers")
    print(f"   FC: {config['neurons']} -> {config['neurons']} -> 3")
    print(f"   Dropout: {config['dropout']}")
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–∞–º—è—Ç—å
    print("üß† –ó–∞–ø–æ–ª–Ω—è–µ–º MLP –ø–∞–º—è—Ç—å...")
    state = train_env.reset()
    for _ in range(800):
        action = np.random.randint(3)
        next_state, _, reward, done = train_env.step(action)
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            state = train_env.reset()
    
    print("üèÉ MLP –æ–±—É—á–µ–Ω–∏–µ —Å –ñ–ò–í–´–ú –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º...")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å TQDM
    best_median = -float('inf')
    
    with tqdm(range(config['episodes']), desc="üî• MLP", 
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
        
        for episode in pbar:
            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
            state = train_env.reset()
            done = False
            total_reward = 0
            
            while not done:
                action = agent.act(state)
                next_state, _, reward, done = train_env.step(action)
                agent.remember(state, action, reward, next_state, done)
                agent.update()
                total_reward += reward
                state = next_state
            
            train_profit = train_env.current_equity - train_env.initial_capital
            
            # OOS —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 16 —ç–ø–∏–∑–æ–¥–æ–≤
            if episode % 16 == 0:
                oos_profits = []
                for start_pos in range(0, min(32, len(oos_env.stock.closes) - config['trading_period'] - config['window']), 8):
                    state = oos_env.reset_fixed(start_pos)
                    done = False
                    
                    while not done:
                        action = agent.act(state, training=False)
                        next_state, _, reward, done = oos_env.step(action)
                        state = next_state
                    
                    oos_profits.append(oos_env.current_equity - oos_env.initial_capital)
                
                oos_median = np.median(oos_profits)
                
                if oos_median > best_median:
                    best_median = oos_median
                    agent.save(f"models/tqdm-mlp-v16_best")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                pbar.set_postfix({
                    'Train': f'${train_profit:.0f}',
                    'OOS_Med': f'${oos_median:.0f}',
                    'Best': f'${best_median:.0f}',
                    'Œµ': f'{agent.epsilon:.3f}',
                    'Trades': f'{train_env.trade_count}'
                })
            else:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ training
                pbar.set_postfix({
                    'Train': f'${train_profit:.0f}',
                    'Œµ': f'{agent.epsilon:.3f}',
                    'Trades': f'{train_env.trade_count}'
                })
            
            time.sleep(0.06)  # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –¥–ª—è MLP
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\nüî¨ MLP —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
    final_profits = []
    
    with tqdm(range(0, min(80, len(oos_env.stock.closes) - config['trading_period'] - config['window']), 5),
              desc="üß™ MLP —Ç–µ—Å—Ç",
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as test_bar:
        
        for start_pos in test_bar:
            state = oos_env.reset_fixed(start_pos)
            done = False
            
            while not done:
                action = agent.act(state, training=False)
                next_state, _, reward, done = oos_env.step(action)
                state = next_state
            
            profit = oos_env.current_equity - oos_env.initial_capital
            final_profits.append(profit)
            
            test_bar.set_postfix({
                'Profit': f'${profit:.0f}',
                'Avg': f'${np.mean(final_profits):.0f}'
            })
    
    # MLP —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    median_profit = np.median(final_profits)
    mean_profit = np.mean(final_profits)
    consistency = len([p for p in final_profits if p > 0]) / len(final_profits) * 100
    
    print(f"\nüî• MLP v16 –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   üí∞ –ú–µ–¥–∏–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏—Ç: ${median_profit:.2f}")
    print(f"   üìà –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ñ–∏—Ç: ${mean_profit:.2f}")  
    print(f"   üéØ Consistency: {consistency:.1f}%")
    print(f"   üèÜ –õ—É—á—à–∏–π –º–µ–¥–∏–∞–Ω–Ω—ã–π: ${best_median:.2f}")
    print(f"   üìä –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {len(final_profits)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    agent.save("models/tqdm-mlp-v16")
    print(f"üíæ MLP –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/tqdm-mlp-v16")
    
    print("üî• MLP v16 –ó–ê–í–ï–†–®–ï–ù!")
    
    return {
        'name': 'MLP v16',
        'median_profit': median_profit,
        'mean_profit': mean_profit,
        'consistency': consistency,
        'best_median': best_median,
        'total_tests': len(final_profits)
    }

if __name__ == "__main__":
    main() 