#!/usr/bin/env python3
"""
üöÄ –ë–´–°–¢–†–´–ô –†–ê–ë–û–ß–ò–ô –¢–ï–°–¢ –° –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú
üéØ –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è!
"""

from src.rl.env import Env
from src.rl.agent import Agent
import numpy as np
from datetime import datetime
from tqdm import tqdm
import time

def main():
    print("üöÄ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ –° –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú!")
    print("üéØ –í–∏–¥–∏–º –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ!")
    print("=" * 50)
    
    # –ë—ã—Å—Ç—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    config = {
        'episodes': 30,        # –±—ã—Å—Ç—Ä–æ –¥–ª—è –¥–µ–º–æ
        'trading_period': 60,  
        'window': 30,          
        'commission': 0.0002,
        
        # –ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        'embeddings': 16,
        'heads': 2,
        'layers': 1,
        'fwex': 64,
        'dropout': 0.1,
        'neurons': 64,
        'lr': 0.001,
        'epsilon': 0.8,
        'epsilon_min': 0.05,
        'epsilon_decay': 0.99,
        'gamma': 0.95,
        'memory_size': 5000,
        'batch_size': 128,
        'update_freq': 5
    }
    
    print(f"üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config['episodes']} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è
    train_env = Env(
        csv_paths=["GOOG_2010-2024-06.csv"],
        fee=config['commission'],
        trading_period=config['trading_period'],
        window=config['window']
    )
    
    oos_env = Env(
        csv_paths=["GOOG_2024-07_2025-04.csv"],
        fee=config['commission'],
        trading_period=config['trading_period'],
        window=config['window']
    )
    
    print(f"üìä –û–∫—Ä—É–∂–µ–Ω–∏—è: Train {train_env.stock.obs_space.shape}, OOS {oos_env.stock.obs_space.shape}")
    
    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent_params = {k: v for k, v in config.items() 
                   if k in ['embeddings', 'heads', 'layers', 'fwex', 'dropout', 'neurons',
                           'lr', 'epsilon', 'epsilon_min', 'epsilon_decay', 'gamma',
                           'memory_size', 'batch_size', 'update_freq']}
    
    agent = Agent(obs_space=train_env.stock.obs_space, **agent_params)
    print(f"ü§ñ –ê–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω! Epsilon: {agent.epsilon:.3f}")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    log_file = "models/quick-working-test.log"
    best_median = -float('inf')
    
    # –û–ë–£–ß–ï–ù–ò–ï –° –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú!
    print(f"\nüèÉ –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º...")
    
    with open(log_file, "w", encoding='utf-8') as f:
        f.write(f"Quick Working Test - {datetime.now()}\n")
        f.write(f"Config: {config}\n")
        f.write("=" * 80 + "\n")
    
    # –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ –° TQDM!
    pbar = tqdm(range(config['episodes']), desc="üß† –û–±—É—á–µ–Ω–∏–µ", 
                ncols=100, colour='green')
    
    for episode in pbar:
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        state = train_env.reset()
        done = False
        total_reward = 0
        steps = 0
        
        while not done:
            action = agent.act(state)
            next_state, trade_action, reward, done = train_env.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            
            if len(agent.memory) > agent.batch_size:
                agent.replay()
            
            total_reward += reward
            state = next_state
            steps += 1
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        train_metrics = train_env.get_trading_metrics()
        
        # OOS —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —ç–ø–∏–∑–æ–¥–æ–≤
        if episode % 5 == 0:
            oos_profits = []
            
            # –ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 3 –ø–æ–∑–∏—Ü–∏—è—Ö
            for start_pos in range(0, min(30, len(oos_env.stock.closes) - config['trading_period'] - config['window']), 10):
                state = oos_env.reset_fixed(start_pos)
                done = False
                
                while not done:
                    action = agent.act(state, training=False)  # –ë–ï–ó exploration
                    next_state, trade_action, reward, done = oos_env.step(action)
                    state = next_state
                
                oos_metrics = oos_env.get_trading_metrics()
                oos_profits.append(oos_metrics['total_profit_dollars'])
            
            median_oos = np.median(oos_profits) if oos_profits else 0
            consistency = (np.array(oos_profits) > 0).mean() * 100 if oos_profits else 0
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if median_oos > best_median:
                best_median = median_oos
                agent.save(f"models/quick-working-test_best")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
            pbar.set_postfix({
                'Train': f"${train_metrics['total_profit_dollars']:.0f}",
                'OOS': f"${median_oos:.0f}",
                'Trades': train_metrics['num_trades'],
                'Win%': f"{train_metrics['win_rate']*100:.1f}",
                'Œµ': f"{agent.epsilon:.3f}",
                'Best': f"${best_median:.0f}"
            })
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            log_entry = (
                f"Ep: {episode:3d} | Train: ${train_metrics['total_profit_dollars']:4.0f} | OOS Med: ${median_oos:4.0f}\n"
                f"    Trades: {train_metrics['num_trades']} | Win%: {train_metrics['win_rate']*100:.1f} | Steps: {steps}\n"
                f"    Consistency: {consistency:.1f}% | Epsilon: {agent.epsilon:.3f} | Best: ${best_median:.0f}\n"
                f"    OOS profits: {[int(p) for p in oos_profits]}\n"
                + "-" * 60
            )
            
            with open(log_file, "a", encoding='utf-8') as f:
                f.write(log_entry + "\n")
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            pbar.set_postfix({
                'Train': f"${train_metrics['total_profit_dollars']:.0f}",
                'Trades': train_metrics['num_trades'],
                'Win%': f"{train_metrics['win_rate']*100:.1f}",
                'Œµ': f"{agent.epsilon:.3f}",
                'Steps': steps
            })
        
        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        time.sleep(0.1)
    
    pbar.close()
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    agent.save(f"models/quick-working-test")
    
    print(f"\nüèÅ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üíæ –ú–æ–¥–µ–ª—å: models/quick-working-test")
    print(f"üèÜ –õ—É—á—à–∞—è: models/quick-working-test_best")
    print(f"üìä –õ–æ–≥: {log_file}")
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print(f"\nüî¨ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º...")
    final_profits = []
    
    test_positions = list(range(0, min(20, len(oos_env.stock.closes) - config['trading_period'] - config['window']), 2))
    
    with tqdm(test_positions, desc="üß™ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç", ncols=80, colour='blue') as test_pbar:
        for start_pos in test_pbar:
            state = oos_env.reset_fixed(start_pos)
            done = False
            
            while not done:
                action = agent.act(state, training=False)
                next_state, trade_action, reward, done = oos_env.step(action)
                state = next_state
            
            oos_metrics = oos_env.get_trading_metrics()
            profit = oos_metrics['total_profit_dollars']
            final_profits.append(profit)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            test_pbar.set_postfix({
                'Profit': f"${profit:.0f}",
                'Avg': f"${np.mean(final_profits):.0f}"
            })
    
    final_median = np.median(final_profits)
    final_consistency = (np.array(final_profits) > 0).mean() * 100
    final_std = np.std(final_profits)
    
    print(f"\nüìà –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   üí∞ –ú–µ–¥–∏–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏—Ç: ${final_median:.2f}")
    print(f"   üéØ Consistency: {final_consistency:.1f}%")
    print(f"   üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: ${final_std:.2f}")
    print(f"   üìà –î–∏–∞–ø–∞–∑–æ–Ω: ${min(final_profits):.0f} - ${max(final_profits):.0f}")
    print(f"   üî• –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: ${best_median:.0f}")
    
    if final_consistency > 50 and final_median > 50:
        print("‚úÖ –°–ò–°–¢–ï–ú–ê –†–ê–ë–û–¢–ê–ï–¢! üéâ")
    else:
        print("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞...")

if __name__ == "__main__":
    main() 