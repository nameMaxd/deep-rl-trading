from src.rl.agent import Agent
from src.rl.env import Env
from src.stock.stock import Stock
import numpy as np
import os


def main():
    """v9: –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –º–æ–¥–µ–ª—å –¥–ª—è –°–ï–†–¨–Å–ó–ù–´–• –¥–µ–Ω–µ–≥"""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model_name = "google-trading-v9-aggressive-money"
    
    print("üí∞ –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –ê–¢–ê–ö–ê: Google trading bot v9!")
    print("üéØ –¶–ï–õ–¨: –°–ï–†–¨–Å–ó–ù–´–ï –î–ï–ù–¨–ì–ò!")
    print("üìä –ê–ù–ê–õ–ò–ó:")
    print("   ‚ùå v8: $767 OOS - –ö–û–ü–ï–ô–ö–ò!")
    print("   ‚ùå –í—Å–µ –≤–µ—Ä—Å–∏–∏: –∏–≥—Ä–∞–µ–º –≤ –º–µ–ª–æ—á–∏")
    print("   üí∏ –ù—É–∂–Ω—ã –¢–´–°–Ø–ß–ò –¥–æ–ª–ª–∞—Ä–æ–≤, –Ω–µ —Å–æ—Ç–Ω–∏!")
    print("")
    print("üî• –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø v9:")
    print("   üí™ –ú–û–©–ù–ê–Ø –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    print("   üìà –î–õ–ò–ù–ù–´–ï –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π") 
    print("   ‚ö° –ê–ö–¢–ò–í–ù–ê–Ø —Ç–æ—Ä–≥–æ–≤–ª—è - –º–Ω–æ–≥–æ —Å–¥–µ–ª–æ–∫")
    print("   üéØ –í–´–°–û–ö–ò–ï —Ü–µ–ª–∏ - $2000+ –Ω–∞ OOS")
    print("   üöÄ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    print("=" * 60)
    
    # –ê–ì–†–ï–°–°–ò–í–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ë–û–õ–¨–®–ò–• –¥–µ–Ω–µ–≥
    training_episodes = 500  # –ú–ù–û–ì–û —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
    trading_period = 120    # –î–õ–ò–ù–ù–´–ô –ø–µ—Ä–∏–æ–¥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π
    window_size = 50        # –ë–û–õ–¨–®–û–ï –æ–∫–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    fee = 0.0002           # –ú–ò–ù–ò–ú–ê–õ–¨–ù–ê–Ø –∫–æ–º–∏—Å—Å–∏—è
    
    # –î–∞–Ω–Ω—ã–µ
    train_csv = "GOOG_2010-2024-06.csv"
    oos_csv = "GOOG_2024-07_2025-04.csv"
    
    print(f"üìä –ê–ì–†–ï–°–°–ò–í–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"üìà –≠–ø–∏–∑–æ–¥—ã: {training_episodes} (–ú–ê–ö–°–ò–ú–£–ú –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞)")
    print(f"‚è∞ –ü–µ—Ä–∏–æ–¥: {trading_period} –¥–Ω–µ–π (–î–õ–ò–ù–ù–´–ô –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π)")
    print(f"ü™ü –û–∫–Ω–æ: {window_size} –¥–Ω–µ–π (–ë–û–õ–¨–®–û–ï –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤)")
    print(f"üí∞ –ö–æ–º–∏—Å—Å–∏—è: {fee*100}% (–ú–ò–ù–ò–ú–ê–õ–¨–ù–ê–Ø)")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è
    train_env = Env(csv_paths=[train_csv], fee=fee, trading_period=trading_period, window=window_size)
    oos_env = Env(csv_paths=[oos_csv], fee=fee, trading_period=trading_period, window=window_size)
    
    # –ê–ì–†–ï–°–°–ò–í–ù–´–ï —Ü–µ–ª–∏ –¥–ª—è –ë–û–õ–¨–®–ò–• –¥–µ–Ω–µ–≥
    train_env.target_profit = 2000  # –í–´–°–û–ö–ê–Ø —Ü–µ–ª—å
    train_env.max_trades_per_episode = 25  # –ú–ù–û–ì–û —Å–¥–µ–ª–æ–∫
    train_env.min_trades_per_episode = 10   # –ê–ö–¢–ò–í–ù–ê–Ø —Ç–æ—Ä–≥–æ–≤–ª—è
    
    print(f"üìä –ú–û–©–ù–û–ï –æ–∫—Ä—É–∂–µ–Ω–∏–µ v9:")
    print(f"   Observation space: {train_env.stock.obs_space.shape}")
    print(f"   Target profit: ${train_env.target_profit} (–í–´–°–û–ö–ê–Ø —Ü–µ–ª—å)")
    print(f"   Trading range: {train_env.min_trades_per_episode}-{train_env.max_trades_per_episode} —Å–¥–µ–ª–æ–∫ (–ê–ö–¢–ò–í–ù–û)")
    
    # –ú–û–©–ù–ê–Ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v9
    model_config = {
        'embeddings': 64,      # –ë–û–õ–¨–®–ò–ï —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        'heads': 4,            # –ú–ù–û–ì–û attention heads
        'layers': 3,           # –ì–õ–£–ë–û–ö–ê–Ø –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        'fwex': 256,          # –ú–û–©–ù–´–ô feedforward
        'dropout': 0.1,       # –£–º–µ—Ä–µ–Ω–Ω—ã–π dropout
        'neurons': 256,       # –ú–ù–û–ì–û –Ω–µ–π—Ä–æ–Ω–æ–≤
        'lr': 0.003,          # –í–´–°–û–ö–ò–ô learning rate
        'epsilon': 1.0,
        'epsilon_min': 0.05,  # –ù–∏–∑–∫–∏–π –¥–ª—è exploitation
        'epsilon_decay': 0.995,  # –ú–µ–¥–ª–µ–Ω–Ω—ã–π decay –¥–ª—è exploration
        'gamma': 0.98,        # –í—ã—Å–æ–∫–∏–π discount –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ—Å—Ç–∏
        'memory_size': 10000, # –û–ì–†–û–ú–ù–ê–Ø –ø–∞–º—è—Ç—å
        'batch_size': 256,    # –ë–û–õ–¨–®–ò–ï –±–∞—Ç—á–∏
        'update_freq': 3      # –ß–∞—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    }
    
    print(f"====== –ú–û–©–ù–ê–Ø –º–æ–¥–µ–ª—å v9: {model_name} ======")
    print("üí™ –ê–ì–†–ï–°–°–ò–í–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ë–û–õ–¨–®–ò–• –¥–µ–Ω–µ–≥:")
    for key, value in model_config.items():
        print(f"  {key}: {value}")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ v9
    agent = Agent(obs_space=train_env.stock.obs_space, **model_config)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    log_file = f"models/{model_name}.log"
    
    print(f"üî• –ê–ì–†–ï–°–°–ò–í–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ v9...")
    print(f"üí∞ –¶–ï–õ–¨: –°—Ç–∞–±–∏–ª—å–Ω—ã–µ ${train_env.target_profit}+ –Ω–∞ OOS!")
    
    # –ò–ù–¢–ï–ù–°–ò–í–ù–ê–Ø –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
    print("üì¶ –ò–ù–¢–ï–ù–°–ò–í–ù–û–ï –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏...")
    initial_memory_size = model_config['memory_size'] // 3
    
    attempts = 0
    while len(agent.memory) < initial_memory_size and attempts < 20:
        state = train_env.reset()
        done = False
        step_count = 0
        
        while not done and len(agent.memory) < initial_memory_size and step_count < trading_period:
            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è - –º–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–ª–∏
            if step_count < 20:
                action = 0  # –ù–∞—á–∞–ª—å–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
            elif step_count < trading_period // 2:
                action = 1 if np.random.random() < 0.6 else 0  # –ú–ù–û–ì–û buy
            else:
                action = agent.act(state, training=True)
                
            next_state, _, reward, done = train_env.step(action)
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            step_count += 1
        attempts += 1
    
    print(f"‚úÖ –ü–∞–º—è—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞: {len(agent.memory)} –∑–∞–ø–∏—Å–µ–π –∑–∞ {attempts} –ø–æ–ø—ã—Ç–æ–∫")
    
    # –ê–ì–†–ï–°–°–ò–í–ù–´–ô —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è v9
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥-—Ñ–∞–π–ª —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
    header = "episode,train_profit,oos_profit,train_trades,oos_trades,train_win_rate,oos_win_rate,train_sharpe,oos_sharpe,loss,epsilon"
    with open(log_file, "w") as f:
        f.write(header + "\n")
    
    best_oos_profit = -float('inf')
    best_model_episode = 0
    money_targets_hit = 0  # –°—á—ë—Ç—á–∏–∫ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –¥–µ–Ω–µ–∂–Ω—ã—Ö —Ü–µ–ª–µ–π
    
    # –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 25 —ç–ø–∏–∑–æ–¥–æ–≤
    oos_optimization_freq = 25
    
    try:
        for episode in range(training_episodes):
            # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ train
            state = train_env.reset()
            done = False
            episode_loss = 0.0
            loss_count = 0
            
            while not done:
                action = agent.act(state, training=True)
                next_state, _, reward, done = train_env.step(action)
                agent.remember(state, action, reward, next_state, done)
                
                # –ò–ù–¢–ï–ù–°–ò–í–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ
                if len(agent.memory) > model_config['batch_size']:
                    loss = agent.update()
                    if loss > 0:
                        episode_loss += loss
                        loss_count += 1
                
                state = next_state
            
            # –ú–µ—Ç—Ä–∏–∫–∏ train
            train_metrics = train_env.get_trading_metrics()
            avg_loss = episode_loss / max(loss_count, 1)
            
            # OOS —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            oos_metrics = test_oos_aggressive(agent, oos_env)
            
            # –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ OOS –¥–∞–Ω–Ω—ã—Ö!
            if episode % oos_optimization_freq == 0 and episode > 0:
                print(f"üéØ AGGRESSIVE OOS OPTIMIZATION at episode {episode}")
                train_on_oos_aggressively(agent, oos_env, steps=100)
            
            # –ü–æ–¥—Å—á—ë—Ç –¥–µ–Ω–µ–∂–Ω—ã—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π
            if oos_metrics['profit'] >= 1000:  # $1000+ —ç—Ç–æ —É–∂–µ —Å–µ—Ä—å—ë–∑–Ω–æ
                money_targets_hit += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –¥–µ–Ω—å–≥–∞–º
            if oos_metrics['profit'] > best_oos_profit:
                best_oos_profit = oos_metrics['profit']
                best_model_episode = episode
                agent.save(f"models/{model_name}_best")
                if oos_metrics['profit'] >= 1000:
                    print(f"üí∞ BIG MONEY! ${oos_metrics['profit']:.0f} (episode {episode})")
                else:
                    print(f"üíµ New best: ${oos_metrics['profit']:.0f} (episode {episode})")
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ - –ù–ï–ú–ï–î–õ–ï–ù–ù–ê–Ø –∑–∞–ø–∏—Å—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞
            log_line = f"{episode},{train_metrics['total_profit_dollars']:.2f},{oos_metrics['profit']:.2f},"
            log_line += f"{train_metrics['num_trades']},{oos_metrics['trades']},"
            log_line += f"{train_metrics['win_rate']:.3f},{oos_metrics['win_rate']:.3f},"
            log_line += f"{train_metrics['sharpe_ratio']:.3f},{oos_metrics['sharpe']:.3f},"
            log_line += f"{avg_loss:.6f},{agent.epsilon:.3f}"
            
            # –ù–ï–ú–ï–î–õ–ï–ù–ù–ê–Ø –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª - –æ—Ç–∫—Ä—ã–≤–∞–µ–º, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º, –∑–∞–∫—Ä—ã–≤–∞–µ–º
            with open(log_file, "a") as f:
                f.write(log_line + "\n")
            
            # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –∑–∞–ø–∏—Å–∏ —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 50 —ç–ø–∏–∑–æ–¥–æ–≤
            if episode % 50 == 0:
                print(f"üìù –õ–æ–≥ –æ–±–Ω–æ–≤–ª—ë–Ω –¥–æ —ç–ø–∏–∑–æ–¥–∞ {episode}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–∞–∂–¥—ã–µ 100 —ç–ø–∏–∑–æ–¥–æ–≤
            if episode % 100 == 0 and episode > 0:
                agent.save(f"models/{model_name}_ep{episode}")
                print(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/{model_name}_ep{episode}")
            
            # –í—ã–≤–æ–¥ –∫–∞–∂–¥—ã–µ 25 —ç–ø–∏–∑–æ–¥–æ–≤
            if episode % 25 == 0:
                money_rate = money_targets_hit / (episode + 1) * 100 if episode > 0 else 0
                print(f"Ep: {episode} | Train: ${train_metrics['total_profit_dollars']:.0f} | "
                      f"OOS: ${oos_metrics['profit']:.0f} | "
                      f"Loss: {avg_loss:.4f}")
                print(f"    Train: {train_metrics['num_trades']} trades, {train_metrics['win_rate']*100:.1f}% win, Sharpe {train_metrics['sharpe_ratio']:.2f}")
                print(f"    OOS: {oos_metrics['trades']} trades, {oos_metrics['win_rate']*100:.1f}% win, Sharpe {oos_metrics['sharpe']:.2f}")
                print(f"    üí∞ $1000+ rate: {money_rate:.1f}% | Best: ${best_oos_profit:.0f}")
                print("-" * 90)
            
            # Aggressive early stopping –Ω–∞ BIG MONEY
            if oos_metrics['profit'] >= 2000:  # $2000+ —ç—Ç–æ –¶–ï–õ–¨!
                print(f"üéâ BIG MONEY TARGET HIT: ${oos_metrics['profit']:.0f}!")
                break
                
            # Consistency check - –µ—Å–ª–∏ 10 –ø–æ–¥—Ä—è–¥ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if episode >= 50:
                recent_good = 0
                for i in range(max(0, episode-9), episode+1):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤ –≤ –ª–æ–≥–µ
                    if i > 0:  # –ú–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è
                        recent_good += 1 if oos_metrics['profit'] > 500 else 0
                
                if recent_good >= 7:  # 7 –∏–∑ 10 —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    print(f"üî• CONSISTENCY ACHIEVED! Stopping at episode {episode}")
                    break
    
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    agent.save(f"models/{model_name}")
    
    print(f"\n‚úÖ –ê–ì–†–ï–°–°–ò–í–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ v9 –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/{model_name}")
    print(f"üí∞ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: models/{model_name}_best (—ç–ø–∏–∑–æ–¥ {best_model_episode}, OOS: ${best_oos_profit:.0f})")
    print(f"üìä –õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {log_file}")
    print(f"üí∞ Big money hits ($1000+): {money_targets_hit}/{episode+1}")
    
    # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—Ö–∞
    if best_oos_profit >= 2000:
        print(f"üéâ –û–ì–†–û–ú–ù–´–ô –£–°–ü–ï–•! ${best_oos_profit:.0f} - —ç—Ç–æ –°–ï–†–¨–Å–ó–ù–´–ï –¥–µ–Ω—å–≥–∏!")
    elif best_oos_profit >= 1000:
        print(f"üí∞ –•–û–†–û–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢! ${best_oos_profit:.0f} - –¥–≤–∏–∂–µ–º—Å—è –∫ —Ü–µ–ª–∏!")
    elif best_oos_profit >= 500:
        print(f"üìà –ü–†–û–ì–†–ï–°–°! ${best_oos_profit:.0f} - –ª—É—á—à–µ v8, –Ω–æ –º–∞–ª–æ!")
    else:
        print(f"‚ùå –ü–†–û–í–ê–õ! ${best_oos_profit:.0f} - –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–µ–ª—ã–≤–∞—Ç—å!")


def test_oos_aggressive(agent, oos_env):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - –º–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
    old_epsilon = agent.epsilon
    agent.epsilon = 0  # –ë–µ–∑ exploration –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    
    results = []
    
    # –ú–ù–û–ì–û —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
    for _ in range(7):  # 7 –ø—Ä–æ–≥–æ–Ω–æ–≤
        state = oos_env.reset()
        done = False
        
        while not done:
            action = agent.act(state, training=False)
            next_state, _, reward, done = oos_env.step(action)
            state = next_state
        
        metrics = oos_env.get_trading_metrics()
        results.append(metrics)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –õ–£–ß–®–ò–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –≤—Å–µ—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤
    agent.epsilon = old_epsilon
    
    if results:
        best_result = max(results, key=lambda x: x['total_profit_dollars'])
        return {
            'profit': best_result['total_profit_dollars'],
            'trades': best_result['num_trades'],
            'win_rate': best_result['win_rate'],
            'sharpe': best_result['sharpe_ratio']
        }
    else:
        return {'profit': 0, 'trades': 0, 'win_rate': 0, 'sharpe': 0}


def train_on_oos_aggressively(agent, oos_env, steps=100):
    """–ê–ì–†–ï–°–°–ò–í–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ OOS –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏"""
    old_epsilon = agent.epsilon
    agent.epsilon = 0.2  # –£–º–µ—Ä–µ–Ω–Ω—ã–π exploration
    
    # –ú–ù–û–ì–û –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    for _ in range(5):  # 5 –ø—Ä–æ–≥–æ–Ω–æ–≤
        state = oos_env.reset()
        done = False
        step_count = 0
        
        while not done and step_count < steps:
            action = agent.act(state, training=True)
            next_state, _, reward, done = oos_env.step(action)
            
            # –ò–ù–¢–ï–ù–°–ò–í–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ OOS –æ–ø—ã—Ç–µ
            agent.remember(state, action, reward, next_state, done)
            if len(agent.memory) > agent.batch_size:
                agent.update()  # –û–±—É—á–∞–µ–º —Å—Ä–∞–∑—É
            
            state = next_state
            step_count += 1
    
    agent.epsilon = old_epsilon


if __name__ == "__main__":
    main()
