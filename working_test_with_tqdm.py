#!/usr/bin/env python3
"""
üöÄ –†–ê–ë–û–ß–ò–ô –¢–ï–°–¢ –° TQDM - –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–û –†–ê–ë–û–¢–ê–ï–¢!
üéØ –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∂–∏–≤–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
"""

from src.rl.env import Env
from src.rl.agent import Agent
import numpy as np
from datetime import datetime
from tqdm import tqdm
import time

def main():
    print("üöÄ –†–ê–ë–û–ß–ò–ô –¢–ï–°–¢ –° –ñ–ò–í–´–ú –ü–†–û–ì–†–ï–°–°–û–ú!")
    print("üéØ –í–∏–¥–∏–º –∫–∞–∫ –∏–¥–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏!")
    print("=" * 60)
    
    # –ë—ã—Å—Ç—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ TQDM
    config = {
        'episodes': 100,       # –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        'trading_period': 60,  
        'window': 30,          
        'commission': 0.0002,
        
        # –ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        'embeddings': 16,
        'heads': 2,
        'layers': 1,
        'fwex': 64,
        'dropout': 0.1,
        'neurons': 64,
        'lr': 0.001,
        'epsilon': 0.8,
        'epsilon_min': 0.05,
        'epsilon_decay': 0.99,
        'gamma': 0.95,
        'memory_size': 5000,
        'batch_size': 128,
        'update_freq': 5
    }
    
    print(f"üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config['episodes']} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è
    train_env = Env(
        csv_paths=["GOOG_2010-2024-06.csv"],
        fee=config['commission'],
        trading_period=config['trading_period'],
        window=config['window']
    )
    
    oos_env = Env(
        csv_paths=["GOOG_2024-07_2025-04.csv"],
        fee=config['commission'],
        trading_period=config['trading_period'],
        window=config['window']
    )
    
    print(f"üìä –û–∫—Ä—É–∂–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"   Train: {train_env.stock.obs_space}")
    print(f"   OOS: {oos_env.stock.obs_space}")
    
    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent = Agent(
        obs_space=train_env.stock.obs_space,
        **{k: v for k, v in config.items() 
           if k in ['embeddings', 'heads', 'layers', 'fwex', 'dropout', 'neurons',
                   'lr', 'epsilon', 'epsilon_min', 'epsilon_decay', 'gamma', 
                   'memory_size', 'batch_size', 'update_freq']}
    )
    
    print("ü§ñ –ê–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω!")
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–∞–º—è—Ç—å
    print("üß† –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–∞–º—è—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    state = train_env.reset()
    for _ in range(500):  # –±—ã—Å—Ç—Ä–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ
        action = np.random.randint(3)
        next_state, _, reward, done = train_env.step(action)
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            state = train_env.reset()
    
    print("üèÉ –û–±—É—á–µ–Ω–∏–µ —Å –ñ–ò–í–´–ú –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º...")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å TQDM
    best_median = -float('inf')
    
    with tqdm(range(config['episodes']), desc="üß† –û–±—É—á–µ–Ω–∏–µ", 
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
        
        for episode in pbar:
            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
            state = train_env.reset()
            done = False
            total_reward = 0
            
            while not done:
                action = agent.act(state)
                next_state, _, reward, done = train_env.step(action)
                agent.remember(state, action, reward, next_state, done)
                agent.update()  # –û–±—É—á–∞–µ–º —Å–µ—Ç—å
                total_reward += reward
                state = next_state
            
            train_profit = train_env.current_equity - train_env.initial_capital
            
            # OOS —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
            if episode % 10 == 0:
                oos_profits = []
                for start_pos in range(0, min(30, len(oos_env.stock.closes) - config['trading_period'] - config['window']), 10):
                    state = oos_env.reset_fixed(start_pos)
                    done = False
                    
                    while not done:
                        action = agent.act(state, training=False)  # –ë–ï–ó exploration
                        next_state, _, reward, done = oos_env.step(action)
                        state = next_state
                    
                    oos_profits.append(oos_env.current_equity - oos_env.initial_capital)
                
                oos_median = np.median(oos_profits)
                
                if oos_median > best_median:
                    best_median = oos_median
                    agent.save(f"models/working-test-tqdm_best")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                pbar.set_postfix({
                    'Train': f'${train_profit:.0f}',
                    'OOS_Med': f'${oos_median:.0f}',
                    'Best': f'${best_median:.0f}',
                    'Œµ': f'{agent.epsilon:.3f}'
                })
            else:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ training —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                pbar.set_postfix({
                    'Train': f'${train_profit:.0f}',
                    'Œµ': f'{agent.epsilon:.3f}'
                })
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
            time.sleep(0.1)
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\nüî¨ –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
    final_profits = []
    
    with tqdm(range(0, min(50, len(oos_env.stock.closes) - config['trading_period'] - config['window']), 5),
              desc="üß™ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç",
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as test_bar:
        
        for start_pos in test_bar:
            state = oos_env.reset_fixed(start_pos)
            done = False
            
            while not done:
                action = agent.act(state, training=False)
                next_state, _, reward, done = oos_env.step(action)
                state = next_state
            
            profit = oos_env.current_equity - oos_env.initial_capital
            final_profits.append(profit)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
            test_bar.set_postfix({
                'Profit': f'${profit:.0f}',
                'Avg': f'${np.mean(final_profits):.0f}'
            })
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    median_profit = np.median(final_profits)
    mean_profit = np.mean(final_profits)
    consistency = len([p for p in final_profits if p > 0]) / len(final_profits) * 100
    
    print(f"\nüéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° TQDM:")
    print(f"   üí∞ –ú–µ–¥–∏–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏—Ç: ${median_profit:.2f}")
    print(f"   üìà –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ñ–∏—Ç: ${mean_profit:.2f}")  
    print(f"   üéØ Consistency: {consistency:.1f}%")
    print(f"   üèÜ –õ—É—á—à–∏–π –º–µ–¥–∏–∞–Ω–Ω—ã–π: ${best_median:.2f}")
    print(f"   üìä –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {len(final_profits)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    agent.save("models/working-test-tqdm")
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/working-test-tqdm")
    
    print("üî• TQDM –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù!")

if __name__ == "__main__":
    main() 