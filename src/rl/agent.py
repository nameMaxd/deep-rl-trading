from src.ml.dqtn import DQTN
import numpy as np
import torch
import torch.nn.functional as F
import random
from collections import deque


class Agent:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π DQN Agent —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º exploration –∏ regularization"""
    
    def __init__(self, obs_space, embeddings=32, heads=1, layers=1, fwex=32,
                 dropout=0.15, neurons=64, lr=0.001, epsilon=1.0, epsilon_min=0.1, 
                 epsilon_decay=0.995, gamma=0.95, memory_size=2000, batch_size=32,
                 update_freq=10):
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.obs_space = obs_space
        self.action_size = 3  # hold, buy, sell
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.gamma = gamma
        self.batch_size = batch_size
        self.update_freq = update_freq
        self.steps = 0
        
        # Adaptive exploration
        self.performance_history = deque(maxlen=50)
        self.epsilon_boost_counter = 0
        self.last_exploration_boost = 0
        
        # Curiosity-driven exploration
        self.action_frequency = {0: 0, 1: 0, 2: 0}
        self.state_visit_count = {}
        
        # –ü–∞–º—è—Ç—å –¥–ª—è –æ–ø—ã—Ç–∞
        self.memory = deque(maxlen=memory_size)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã observation space
        if len(obs_space.shape) == 3:
            num_features, seq_len = obs_space.shape[1], obs_space.shape[2]
        else:
            num_features, seq_len = obs_space.shape[0], obs_space.shape[1]
        
        print(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ v5:")
        print(f"   Observation space: {obs_space.shape}")
        print(f"   Features: {num_features}, Sequence: {seq_len}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.q_network = DQTN(
            embeddings=embeddings,
            heads=heads,
            layers=layers,
            fwex=fwex,
            dropout=dropout,  # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–ª–∏ dropout
            neurons=neurons,
            lr=lr,
            view_size=seq_len
        )
        
        self.device = self.q_network.device
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π epsilon: {epsilon} -> {epsilon_min}")
        print(f"   Curiosity exploration: –í–∫–ª—é—á–µ–Ω–æ")

    def remember(self, state, action, reward, next_state, done):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞–º—è—Ç—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        if len(self.memory) > 0:
            last_state, _, _, _, _ = self.memory[-1]
            state_similarity = np.corrcoef(state.flatten(), last_state.flatten())[0, 1]
            if state_similarity > 0.99:  # –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
                return
                
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, training=True):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º exploration"""
        if training:
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π epsilon –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            current_epsilon = self._get_adaptive_epsilon()
            
            if np.random.random() <= current_epsilon:
                # Curiosity-driven exploration
                action = self._curiosity_action(state)
            else:
                # Exploitation —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å
                action = self._greedy_action(state)
        else:
            # –í —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ —Ç–æ–ª—å–∫–æ exploitation
            action = self._greedy_action(state)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.action_frequency[action] += 1
        
        return action

    def _get_adaptive_epsilon(self):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π epsilon –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        base_epsilon = self.epsilon
        
        # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–ª–æ—Ö–∞—è, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º exploration
        if len(self.performance_history) >= 10:
            recent_performance = np.mean(list(self.performance_history)[-10:])
            if recent_performance < -0.01:  # –ü–ª–æ—Ö–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                boost_factor = min(2.0, 1.0 + abs(recent_performance) * 10)
                return min(0.8, base_epsilon * boost_factor)
        
        return base_epsilon

    def _curiosity_action(self, state):
        """Curiosity-driven exploration - –≤—ã–±–∏—Ä–∞–µ–º –º–µ–Ω–µ–µ —á–∞—Å—Ç—ã–µ –¥–µ–π—Å—Ç–≤–∏—è"""
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        min_freq = min(self.action_frequency.values())
        least_used_actions = [action for action, freq in self.action_frequency.items() 
                             if freq == min_freq]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
        if np.random.random() < 0.3:
            return random.choice(least_used_actions)
        else:
            return random.randrange(self.action_size)

    def _greedy_action(self, state):
        """–ñ–∞–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å"""
        q_values = self.q_network.predict(state)
        return np.argmax(q_values)

    def replay(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å regularization"""
        if len(self.memory) < self.batch_size:
            return 0.0
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –±–∞—Ç—á
        batch = random.sample(self.memory, self.batch_size)
        
        states = np.array([e[0] for e in batch])
        actions = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])
        
        # –û–±—É—á–∞–µ–º —Å–µ—Ç—å
        loss = self.q_network.train_on_batch(
            states, actions, rewards, next_states, dones, self.gamma
        )
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ epsilon
        self._update_epsilon_adaptive(np.mean(rewards))
        
        return loss

    def _update_epsilon_adaptive(self, batch_reward):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ epsilon"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        self.performance_history.append(batch_reward)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # –û–¢–ö–õ–Æ–ß–ê–ï–ú adaptive epsilon boost - –æ–Ω –ø–æ—Ä—Ç–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!
        # if len(self.performance_history) >= 20:
        #     recent_avg = np.mean(list(self.performance_history)[-10:])
        #     older_avg = np.mean(list(self.performance_history)[-20:-10])
        #     
        #     if recent_avg <= older_avg and self.steps - self.last_exploration_boost > 100:
        #         self.epsilon = min(0.5, self.epsilon * 1.2)  # Boost exploration
        #         self.last_exploration_boost = self.steps
        #         print(f"üîç Exploration boost! Epsilon: {self.epsilon:.3f}")

    def update(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞"""
        self.steps += 1
        
        # –ë–æ–ª–µ–µ —á–∞—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if self.steps % self.update_freq == 0:
            return self.replay()
        
        return 0.0

    def get_exploration_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ exploration"""
        total_actions = sum(self.action_frequency.values())
        if total_actions == 0:
            return {"hold": 0, "buy": 0, "sell": 0}
        
        return {
            "hold": self.action_frequency[0] / total_actions,
            "buy": self.action_frequency[1] / total_actions, 
            "sell": self.action_frequency[2] / total_actions
        }

    def save(self, filepath):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        torch.save({
            'model_state_dict': self.q_network.state_dict(),
            'optimizer_state_dict': self.q_network.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'steps': self.steps,
            'performance_history': list(self.performance_history),
            'action_frequency': self.action_frequency
        }, filepath)
        print(f"üíæ –ú–æ–¥–µ–ª—å v5 —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")

    def load(self, filepath):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            self.q_network.load_state_dict(checkpoint['model_state_dict'])
            self.q_network.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.steps = checkpoint.get('steps', 0)
            
            if 'performance_history' in checkpoint:
                self.performance_history = deque(checkpoint['performance_history'], maxlen=50)
            if 'action_frequency' in checkpoint:
                self.action_frequency = checkpoint['action_frequency']
                
            print(f"üì• –ú–æ–¥–µ–ª—å v5 –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def get_action_name(self, action):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è"""
        action_names = {0: "HOLD", 1: "BUY", 2: "SELL"}
        return action_names.get(action, "UNKNOWN")

    def get_stats(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–≥–µ–Ω—Ç–∞"""
        exploration_stats = self.get_exploration_stats()
        
        return {
            'epsilon': self.epsilon,
            'memory_size': len(self.memory),
            'steps': self.steps,
            'device': str(self.device),
            'performance_trend': np.mean(list(self.performance_history)[-10:]) if len(self.performance_history) >= 10 else 0,
            'action_distribution': exploration_stats,
            'exploration_balance': min(exploration_stats.values()) / max(exploration_stats.values()) if max(exploration_stats.values()) > 0 else 0
        }
