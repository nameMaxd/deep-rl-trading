import torch
import torch.nn as nn
import numpy as np
import math


class DQTN(nn.Module):
    def __init__(self, embeddings, heads, layers, fwex, dropout, neurons, lr=0.001, view_size=30):
        super(DQTN, self).__init__()
        
        self.view_size = view_size
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
        self.input_size = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–º forward
        self.embeddings = embeddings
        
        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)
        self.input_embedder = None
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.pos_encoder = PositionalEncoding(embeddings, dropout, max_len=view_size * 20)
        
        # Transformer encoder
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=embeddings,
            nhead=heads,
            dim_feedforward=fwex,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=layers)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
        self.layer_norm = nn.LayerNorm(embeddings)
        self.fc_layers = nn.Sequential(
            nn.Linear(embeddings, neurons),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(neurons, neurons // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(neurons // 2, 3)  # 3 –¥–µ–π—Å—Ç–≤–∏—è: hold, buy, sell
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
        
        self.optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=0.01)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ Xavier"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def _create_input_embedder(self, num_features):
        """–°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–¥–µ—Ä –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∏—á–µ–π"""
        self.input_embedder = nn.Linear(num_features, self.embeddings)
        nn.init.xavier_uniform_(self.input_embedder.weight)
        if self.input_embedder.bias is not None:
            nn.init.zeros_(self.input_embedder.bias)
        self.input_embedder.to(self.device)

    def forward(self, x):
        """
        Forward pass –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        x shape: (batch_size, num_features, sequence_length)
        """
        batch_size, num_features, seq_len = x.shape
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–¥–µ—Ä –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        if self.input_embedder is None:
            self._create_input_embedder(num_features)
            print(f"üß† –°–æ–∑–¥–∞–Ω —ç–º–±–µ–¥–¥–µ—Ä: {num_features} —Ñ–∏—á–µ–π -> {self.embeddings} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è (batch_size, seq_len, num_features)
        x = x.transpose(1, 2)
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.input_embedder(x)  # (batch_size, seq_len, embeddings)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        embeddings = self.pos_encoder(embeddings)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
        transformer_out = self.transformer(embeddings)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        last_state = self.layer_norm(transformer_out[:, -1, :])
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
        output = self.fc_layers(last_state)
        
        return output

    def predict(self, state):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        self.eval()
        with torch.no_grad():
            if isinstance(state, np.ndarray):
                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            else:
                state = state.unsqueeze(0).to(self.device)
            
            q_values = self(state)
            return q_values.cpu().numpy()[0]

    def train_on_batch(self, states, actions, rewards, next_states, dones, gamma=0.9):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ –¥–∞–Ω–Ω—ã—Ö"""
        self.train()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # –¢–µ–∫—É—â–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è
        current_q_values = self(states).gather(1, actions.unsqueeze(1))
        
        # –°–ª–µ–¥—É—é—â–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è (Double DQN)
        with torch.no_grad():
            next_q_values = self(next_states).max(1)[0]
            target_q_values = rewards + (gamma * next_q_values * ~dones)
        
        # Huber loss
        loss = nn.SmoothL1Loss()(current_q_values.squeeze(), target_q_values)
        
        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        self.optimizer.zero_grad()
        loss.backward()
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ
        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        return loss.item()


class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:seq_len, :].transpose(0, 1)
        return self.dropout(x)
